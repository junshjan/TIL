+ 입력층에서 1층으로 신호전달

  + <img src = "https://t1.daumcdn.net/cfile/tistory/9985BB375A692BC72D">

    + 은닉층에서의 가중치 합(가중 신호와 편향의 총합)을 a로 표기하고 활성화 함수 h()로 변환된 신호를 z로 표기

    

+ 1층에서 2층으로의 신호전달

  + <img src = "https://user-images.githubusercontent.com/76824611/117392911-f5f81f00-af2d-11eb-93c7-df5e04b78976.png">
    + 입력층에서 1층으로 신호전달과 동일한 형태로 신호전달



+ 구현 코드

  ```python
  import numpy as np
  
  def sigmoid(x):
      return 1/(1+np.exp(-x))
  
  def identity_function(x):
      return x
  
  def init_network():
      network = {}
      network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
      network['b1'] = np.array([0.1, 0.2, 0.3])
      network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
      network['b2'] = np.array([0.1, 0.2])
      network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])
      network['b3'] = np.array([0.1, 0.2])
  
      return network
  
  
  def forward(network, x):
      W1, W2, W3 = network['W1'], network['W2'], network['W3']
      b1, b2, b3 = network['b1'], network['b2'], network['b3']
  
      a1 = np.dot(x, W1) + b1
      z1 = sigmoid(a1)
      a2 = np.dot(z1, W2) + b2
      z2 = sigmoid(a2)
      a3 = np.dot(z2, W3) + b3
      y = identity_function(a3)
  
      return y
  
  network = init_network()
  x = np.array([1.0, 0.5])
  y = forward(network, x)
  
  print(y)
  ```



# 출력층 설계

#### 항등함수와 소프트맥스 함수 구현

+ 항등함수: 입력을 그대로 출력하는함수
  + <img src ="https://media.vlpt.us/images/chandni_ml/post/be34f559-d01d-4472-a1f0-f0fc150fb8ee/fig%203-21.png">

+ 소프트맥스 함수
  + <img src = "https://t1.daumcdn.net/cfile/tistory/995AFB465A6AA10102">
  + exp(x)는 e^x을 뜻하는 지수함수(e는 자연상수)
  + n은 출력층 뉴런수
  + Y_k는 그중 k번째출력임을 뜻함
  + 소프트맥스 함수의 분자는 입력신호 a_k의 지수함수, 분모는 모든 입력 신호의 지수함수의 합으로 구성된다
  + <img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRwbmCCK4cBaEw9qU9xL0_BJW1MW0lvRx1w0g&usqp=CAU">
  + 출력승의 각 뉴런이 모든 입력신호에서 영향을 받는다

#### 소프트맥스 함수 구현 시 주의점

+ 소프트맥스함수를 컴퓨터로 계산할때 오버플로 문제가 나타날 수 있다

+ 소프트맥스함수를 개선해 이 문제를 해결할 수 있다

  <img src = "https://blog.kakaocdn.net/dn/bhbRR0/btranbZ693E/TkCQnYxo9crNfzPRxKTDf0/img.png">

  + ```
    def softmax(a):
        c = np.max(a)
        exp_a = np.exp(a-c)
        sum_exp_a = np.sum(exp_a)
        y = exp_a / sum_exp_a
        
        return y
    
    ```



#### 소프트맥스 함수의 특징

+ 소프트맥스 함수의 출력은 0에서 1.0 사이의 실수이다
+ 소프트맥스 함수 출력의 총 합이 1이다
  + 이성질을 이용해서 함수의 출력을 확률로 해석할 수 있다
+ 소프트맥스함수를 사용해도 원소의 대소관계는 변하지 않는다
  + 신경망으로 분류할 때는 출력층 소프트맥스 함수를 생략 해도 된다
  + 현업에서도 자원낭비를 줄이고자 출력층의 소프트맥스 함수는 생략하는것이 일반적이다



#### 출력층의 뉴런 수 정하기

출력층의 뉴런 수는 풀려는 문제에 맞게 적절히 정해야한다



#### 손글씨 숫자 인식

이미 학습된 매개변수를 사용해 학습과정은 생력함

추론 과정만 구현 : 순전파



#### MNIST 데이터셋

+ MNIST 데이터셋은 0부터 9까지의 숫자 이미지로 구성됨

